#!/usr/bin/env python3
"""
GPUè®­ç»ƒè„šæœ¬ - ç‹¬ç«‹è®­ç»ƒæ¯ä¸ªåœºæ™¯
ä½¿ç”¨æ–¹å¼ï¼š
    python train_gpu.py --scenario A --num_episodes 1000 
    python train_gpu.py --scenario B --num_episodes 1000 
    python train_gpu.py --scenario C --num_episodes 1000 
"""

import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ä»¥æ”¯æŒç›¸å¯¹å¯¼å…¥
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)
import json
import argparse
import time
import random
import torch
import numpy as np
from datetime import datetime

from configs.config import RPSConfig, SCENARIOS
from envs import VectorizedRPSEnv
from agents import MAPPO_GPU, GPURolloutBuffer, PowerEnterpriseAgentV4


class GPUScenarioTrainer:
    """GPUåœºæ™¯è®­ç»ƒå™¨"""
    
    def __init__(self, scenario_name: str, num_episodes: int = 100):
       
        self.scenario_name = scenario_name
        self.num_episodes = num_episodes
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_parallel_envs = 1  # å›ºå®šä¸ºå•ç¯å¢ƒ
        
        # è®¾ç½®éšæœºç§å­ä¸º42ï¼ˆå›ºå®šï¼‰
        self.seed = 42
        self._set_seeds(self.seed)
        
        # æ£€æŸ¥åœºæ™¯æœ‰æ•ˆæ€§
        if scenario_name not in SCENARIOS:
            raise ValueError(f"Unknown scenario: {scenario_name}. Available: {list(SCENARIOS.keys())}")
        
        print(f"[GPU Trainer] Device: {self.device}")
        print(f"[GPU Trainer] Scenario: {scenario_name} ({SCENARIOS[scenario_name]['description']})")
        print(f"[GPU Trainer] Seed: {self.seed}")
        
    def build_config(self) -> RPSConfig:
        """æ„å»ºè®­ç»ƒé…ç½®"""
        scenario_cfg = SCENARIOS[self.scenario_name]
        
        config = RPSConfig()
        # ç¯å¢ƒé…ç½®
        config.environment.num_agents = config.environment.num_agents  # ä½¿ç”¨configé»˜è®¤å€¼(100)
        config.environment.max_periods = 11  
        config.environment.base_year = 2020
        config.environment.include_caq_market = scenario_cfg['include_caq_market']
        config.environment.include_tgc_market = scenario_cfg['include_tgc_market']
        
        # æ”¿ç­–é…ç½®
        config.policy.fine_per_unit = scenario_cfg['fine_per_unit']
        config.policy.reward_per_unit = scenario_cfg['reward_per_unit']
        
        # è®­ç»ƒé…ç½® (GPUä¼˜åŒ–)
        config.training.rollout_length = 2048
        config.training.hidden_dims = [256, 256]  # æ›´å¤§ç½‘ç»œ
        config.training.batch_size = 128
        config.training.num_epochs = 10
        config.training.lr_actor = 3e-4
        config.training.lr_critic = 3e-4
        config.training.gamma = 0.99
        config.training.gae_lambda = 0.95
        config.training.clip_epsilon = 0.2
        config.training.value_loss_coef = 0.5
        config.training.entropy_coef = 0.01
        config.training.max_grad_norm = 0.5
        
        # GPUé…ç½®
        config.gpu.num_parallel_envs = self.num_parallel_envs
        config.gpu.use_mixed_precision = False  # ç¦ç”¨æ··åˆç²¾åº¦ä»¥æé«˜å…¼å®¹æ€§
        config.gpu.pin_memory = True
        
        return config
    
    def _set_seeds(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        config = self.build_config()
        
        # åˆ›å»ºagent manager
        agent_manager = PowerEnterpriseAgentV4(
            agent_config=config.agent,
            env_config=config.environment,
            market_config=config.market,
            gpu_config=config.gpu,
        )
        
        # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
        env = VectorizedRPSEnv(
            num_envs=self.num_parallel_envs,
            num_agents=config.environment.num_agents,
            max_periods=config.environment.max_periods,
            base_year=config.environment.base_year,
            device=self.device,
            agent_manager=agent_manager,
            fine_per_unit=config.policy.fine_per_unit,
            reward_per_unit=config.policy.reward_per_unit,
            include_caq_market=config.environment.include_caq_market,
            include_tgc_market=config.environment.include_tgc_market,
        )
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = MAPPO_GPU(
            obs_dim=env.obs_dim,
            action_dim=env.action_dim,
            global_state_dim=env.global_state_dim,
            num_agents=config.environment.num_agents,
            num_envs=self.num_parallel_envs,
            hidden_dims=config.training.hidden_dims,
            lr_actor=config.training.lr_actor,
            lr_critic=config.training.lr_critic,
            gamma=config.training.gamma,
            gae_lambda=config.training.gae_lambda,
            clip_epsilon=config.training.clip_epsilon,
            value_loss_coef=config.training.value_loss_coef,
            entropy_coef=config.training.entropy_coef,
            max_grad_norm=config.training.max_grad_norm,
            use_attention=config.training.use_attention,
            use_mixed_precision=config.gpu.use_mixed_precision,
            device=self.device,
        )
        
        # åˆ›å»ºbuffer
        buffer = GPURolloutBuffer(
            buffer_size=config.environment.max_periods,
            num_envs=self.num_parallel_envs,
            num_agents=config.environment.num_agents,
            obs_dim=env.obs_dim,
            action_dim=env.action_dim,
            global_state_dim=env.global_state_dim,
            device=self.device,
        )
        
        # è®­ç»ƒå†å²
        episode_rewards = []
        policy_losses = []
        value_losses = []
        
        start_time = time.time()
        
        for episode in range(self.num_episodes):
            ep_start = time.time()
            
            # é‡ç½®ç¯å¢ƒ
            obs, global_state = env.reset()
            buffer.reset()
            
            episode_reward = torch.zeros(self.num_parallel_envs, device=self.device)
            
            # è¿è¡Œä¸€ä¸ªå®Œæ•´episode
            for step in range(config.environment.max_periods):
                # è·å–åŠ¨ä½œ
                with torch.no_grad():
                    actions, log_probs, values = agent.get_action(obs, global_state)
                
                # ç¯å¢ƒæ­¥è¿›
                next_obs, next_global_state, rewards, dones, infos = env.step(actions)
                
                # å­˜å‚¨åˆ°buffer
                buffer.add(
                    obs=obs,
                    global_state=global_state,
                    action=actions,
                    reward=rewards,
                    done=dones,
                    log_prob=log_probs,
                    value=values,
                )
                
                episode_reward += rewards.sum(dim=1)  # ç´¯ç§¯å¥–åŠ±
                
                obs = next_obs
                global_state = next_global_state
                
                if dones.any():
                    break
            
            # è®¡ç®—GAE
            with torch.no_grad():
                _, _, last_value = agent.get_action(obs, global_state)
            buffer.compute_gae(last_value, config.training.gamma, config.training.gae_lambda)
            
            # æ›´æ–°æ™ºèƒ½ä½“
            metrics = agent.update(
                buffer,
                batch_size=config.training.batch_size,
                num_epochs=config.training.num_epochs,
            )
            
            # è®°å½•
            avg_reward = episode_reward.mean().item()
            episode_rewards.append(avg_reward)
            policy_losses.append(metrics['policy_loss'])
            value_losses.append(metrics['value_loss'])
            
            ep_time = time.time() - ep_start
            
            # æ¯ä¸ªepisodeéƒ½æ‰“å°å¥–åŠ±
            if (episode + 1) % 1 == 0:
                print(f"  [{self.scenario_name}] Episode {episode+1:4d}/{self.num_episodes}: "
                      f"Reward={avg_reward:10.2f}")
            
            # æ¯10ä¸ªepisodeä¿å­˜ä¸€æ¬¡ä¸­é—´æ£€æŸ¥ç‚¹
            if (episode + 1) % 10 == 0:
                checkpoint_dir = f'./gpu_models/{self.scenario_name}_checkpoints'
                os.makedirs(checkpoint_dir, exist_ok=True)
                intermediate_checkpoint = os.path.join(
                    checkpoint_dir,
                    f'{self.scenario_name}_episode_{episode+1:04d}.pt'
                )
                agent.save(intermediate_checkpoint)
        
        total_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        results = {
            'episode_rewards': episode_rewards,
            'policy_losses': policy_losses,
            'value_losses': value_losses,
            'elapsed_time': total_time,
            'avg_reward': float(np.mean(episode_rewards)),
            'final_reward': float(episode_rewards[-1]),
            'timestamp': datetime.now().isoformat(),
        }
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        checkpoint_dir = f'./gpu_models/{self.scenario_name}_checkpoints'
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        os.makedirs('./gpu_models', exist_ok=True)
        final_checkpoint = os.path.join(
            './gpu_models', 
            f'{self.scenario_name}_{self.num_episodes}ep_{datetime.now().strftime("%Y%m%d_%H%M%S")}.pt'
        )
        agent.save(final_checkpoint)
        
        # ä¿å­˜è®­ç»ƒå…ƒæ•°æ®å’Œå¥–åŠ±æ•°æ®
        metadata_path = final_checkpoint.replace('.pt', '_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        # ä¿å­˜å¥–åŠ±æ•°æ®ä¾›åç»­ç»˜åˆ¶
        rewards_path = final_checkpoint.replace('.pt', '_rewards.json')
        rewards_data = {
            'episode_rewards': episode_rewards,
            'policy_losses': [float(x) for x in policy_losses],
            'value_losses': [float(x) for x in value_losses],
        }
        with open(rewards_path, 'w') as f:
            json.dump(rewards_data, f, indent=2)
        
        
        
        return results


# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description='GPU Scenario Training')
parser.add_argument('--scenario', type=str, default='A', 
                    help='Scenario name (A, A1-A7, B, C)')
parser.add_argument('--num_episodes', type=int, default=100, 
                    help='Number of training episodes (default: 100 for quick debugging)')

args = parser.parse_args()

# åˆ›å»ºè®­ç»ƒå™¨å¹¶æ‰§è¡Œ
trainer = GPUScenarioTrainer(
    scenario_name=args.scenario,
    num_episodes=args.num_episodes,
)

results = trainer.train()

# æ‰“å°æ‘˜è¦
print("\n" + "="*80)
print(f"âœ… è®­ç»ƒå®Œæˆï¼")
print(f"   åœºæ™¯: {args.scenario}")
print(f"   è½®æ•°: {args.num_episodes}")
print(f"   å¹³å‡å¥–åŠ±: {results['avg_reward']:.2f}")
print(f"   æœ€ç»ˆå¥–åŠ±: {results['final_reward']:.2f}")
print(f"   è€—æ—¶: {results['elapsed_time']:.1f}s")
print("\nğŸ“Š ç»˜åˆ¶å¥–åŠ±æ›²çº¿:")
print(f"   python plot_reward_from_checkpoint.py --scenario {args.scenario} --episode {args.num_episodes}")
print("="*80 + "\n")